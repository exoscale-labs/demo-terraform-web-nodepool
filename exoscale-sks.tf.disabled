locals {
  labels = { "project" : "sks-demo" }
}

# This resource will create the control plane
# Since we're going for the fully managed option, we will ask sks to preinstall
# the calico network plugin and the exoscale-cloud-controller
resource "exoscale_sks_cluster" "demo" {
  zone           = var.zone
  name           = "demo"
  # version        = "1.23.3"
  description    = "Demo cluster"
  service_level  = "starter"
  cni            = "calico"
  exoscale_ccm   = true
  metrics_server = true
  #auto_upgrade   = false
  labels         = local.labels
}

# A security group so the nodes can communicate and we can pull logs
resource "exoscale_security_group" "sks_nodes" {
  name        = "sks_nodes_terraform"
  description = "Allows traffic between sks nodes and public pulling of logs"
}

resource "exoscale_security_group_rule" "sks_nodes_logs_rule" {
  security_group_id = exoscale_security_group.sks_nodes.id
  type              = "INGRESS"
  protocol          = "TCP"
  start_port        = 10250
  end_port          = 10250
  user_security_group_id = exoscale_security_group.sks_nodes.id
}

resource "exoscale_security_group_rule" "sks_nodes_calico" {
  security_group_id      = exoscale_security_group.sks_nodes.id
  type                   = "INGRESS"
  protocol               = "UDP"
  start_port             = 4789
  end_port               = 4789
  user_security_group_id = exoscale_security_group.sks_nodes.id
}

resource "exoscale_security_group_rule" "sks_nodes_ccm" {
  security_group_id = exoscale_security_group.sks_nodes.id
  type              = "INGRESS"
  protocol          = "TCP"
  start_port        = 30000
  end_port          = 32767
  cidr              = "0.0.0.0/0"
}

resource "exoscale_anti_affinity_group" "sks_workers" {
  name        = "SKS Workers"
  description = "Ensure different hypervisors for SKS workers"
}


# This provisions an instance pool of nodes which will run the kubernetes
# workloads.
# We can attach multiple nodepools to the cluster
resource "exoscale_sks_nodepool" "workers" {
  zone               = var.zone
  cluster_id         = exoscale_sks_cluster.demo.id
  name               = "workers"
  description        = "first nodepool"
  disk_size          = 20
  instance_type      = "standard.small"
  instance_prefix    = "sks-pool"
  size               = 2
  security_group_ids = [exoscale_security_group.sks_nodes.id]
  anti_affinity_group_ids =[exoscale_anti_affinity_group.sks_workers.id]
  labels             = local.labels
}

output "kubectl_command" {
  value = "exo compute sks kubeconfig ${exoscale_sks_cluster.demo.name} kube-admin -z ${var.zone} --group system:masters > ~/.kube/config"
}

# data "external" "kubeconfig" {
#   program = ["./get-kubeconfig.sh"]
#   query = {
#     id = exoscale_sks_cluster.demo.id
#     zone = local.zone
#   }
# }

# # Add the kubeconfig as an output
# output "kubeconfig" {
#   value = data.external.kubeconfig.result.value
# }